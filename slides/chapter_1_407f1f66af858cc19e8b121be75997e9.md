---
title: Insert title here
key: 407f1f66af858cc19e8b121be75997e9

---
## Fit Logistic Regression

```yaml
type: "TitleSlide"
key: "6daa5ec860"
```

`@lower_third`

name: Sundar Krishnan
title: Lead Data Scientist


`@script`
In the previous lecture, we went through the concept of logistic regression and how it can be used to predict binary outcomes. In this lecture, we will understand how to write a python code to train a logistic regression model.


---
## Data

```yaml
type: "FullCodeSlide"
key: "d595d3eee5"
```

`@part1`
The data for this exercise is taken from [UCI Machine learning repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing).  

**Load Data**{{1}}
```
import pandas as pd
df = pd.read_excel("bank.xlsx")
```
{{1}}

**Sneak peek of the data**{{2}}
```
df.head()
```{{2}}
![df.head](https://github.com/Sundar0989/datacamp/blob/master/Data%20-%20Sneak%20Peek.png){{2}}


`@script`
The data for this exercise is taken from UCI Machine learning repository. You can read the description for each input variables in the link attached. To load the data into python, we use pandas 'read_excel' option as shown here. Once loaded, we can use the df.head() function to take a sneak peek look of the data.


---
## Information about the data

```yaml
type: "FullCodeSlide"
key: "044e6d48e9"
```

`@part1`
- 7 Continuous input variables (int64)
- 9 Categorical input variables (object)
- 1 target variable  - **'y'** (object) 
 
```
df.info()
```{{1}}
![df.info](https://github.com/Sundar0989/datacamp/blob/master/Data%20-%20Information.png){{1}}


`@script`
That is cool, but what if I need more detailed information about the data. You can use df.info() function to do that.


---
## What we need to predict?

```yaml
type: "FullCodeSlide"
key: "700714b05b"
```

`@part1`
**Predict** - Whether the client subscribed for a term deposit (yes/no)?

{{1}}
```
df['target'] = df['y'].apply(lambda x: 1 if x == 'yes' else 0)
```{{1}}

**Count of each levels**{{2}}
```
df.target.value_counts()
```{{2}}
**Drop the categorical target column**{{3}}
```
df.drop('y',axis=1,inplace=True)
```{{3}}


`@script`
In this tutorial, we are predicting whether a client subscribes for a term deposit or not. A little preparation is needed for our target column here. Let us convert, the yes/no level to 1 and 0 respectively. Once it is done, we can take a look at the distribution of 1 and 0 using value_counts() function.


---
## Packages to fit logistic regression

```yaml
type: "FullSlide"
key: "c3ab27aa49"
```

`@part1`
1. Statsmodels
2. Sklearn (Widely used)

**Note:** The input data should be **continuous**.{{1}}


`@script`
In python, we can use two packages to train a logistic regression model. The first one is statsmodels package and second one is widely used sklearn package. The input variables to both the packages will accept only continuous variable. Then, what should we do to incorporate categorical variables in the fi


---
## Train/Test split (Honest Assessment)

```yaml
type: "FullCodeSlide"
key: "e1e204df6d"
```

`@part1`
```
from sklearn.model_selection import train_test_split

X = df[['age','balance','day','duration','campaign','pdays','previous']]
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, 
random_state=42)
```{{1}}

The train and test dataset has 3,029 and 1,492 observations respectively.{{2}}
```
len(X_train)
len(X_test)
```{{2}}


`@script`



---
## Statsmodel fit

```yaml
type: "FullCodeSlide"
key: "c3fa5f548a"
```

`@part1`
```
import statsmodels.api as sm

logit = sm.Logit(y_train, X_train).fit()
logit.summary()
```{{1}}
![statsmodel](https://github.com/Sundar0989/datacamp/blob/master/Statsmodels%20Logit%20fit.png){{1}}


`@script`



---
## Sklearn fit

```yaml
type: "FullCodeSlide"
key: "57bc1a5edb"
```

`@part1`
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()

clf.fit(X_train,y_train)
```{{1}}

```
import numpy as np
pd.DataFrame(np.transpose(clf.coef_),index=X_train.columns,columns=['coef'])
```{{2}}
![sklearn](https://github.com/Sundar0989/datacamp/blob/master/Sklearn%20logit%20fit.png){{2}}


`@script`



---
## Comparison of Fit outputs

```yaml
type: "TwoColumns"
key: "6c03b8f7d0"
```

`@part1`
**Statsmodel Coef**

![statsmodel](https://github.com/Sundar0989/datacamp/blob/master/Statsmodel%20logit%20coef.png)


`@part2`
**Sklearn Coef**

![sklearn](https://github.com/Sundar0989/datacamp/blob/master/Sklearn%20logit%20fit.png)


`@script`



---
## Why the fit coefficients are different?

```yaml
type: "FullSlide"
key: "3b3cf8593a"
```

`@part1`
By default, _sklearn_ fits an intercept which _statsmodel_ package does not. In addition, sklearn uses regularization by default.

![fit_intercept](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20fit%20intercept.png)

To bring consistency, one of these options will work{{1}}

1. Set **fit_intercept=False** in sklearn model (This is not recommended){{1}}
2. Add a **constant** in the X_train dataset before fitting statsmodel{{1}}
3. Using a large C value or changing the regularization method or try both{{1}}


`@script`



---
## Sklearn model without fit intercept (Option 1)

```yaml
type: "TwoColumns"
key: "a05fc5b150"
```

`@part1`
**Statsmodel fit**
```
import statsmodels.api as sm

logit = sm.Logit(y_train, X_train).fit()
logit.summary()
```
![statsmodel](https://github.com/Sundar0989/datacamp/blob/master/Statsmodel%20logit%20coef.png)


`@part2`
**sklearn fit without intercept**
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(fit_intercept=False)

clf.fit(X_train,y_train)
```
![sklearn](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20fit%20intercept%20false.png)


`@script`



---
## Updated statsmodel fit and sklearn model (Option 2 and 3)

```yaml
type: "TwoColumns"
key: "c20a35a6ed"
```

`@part1`
**Statsmodel**
```
import statsmodels.api as sm

new_train = sm.add_constant(X_train)
logit = sm.Logit(y_train, new_train).fit()
logit.summary()
```
![statsmdoel](https://github.com/Sundar0989/datacamp/blob/master/statsmodel%20const%20fit%20logit.png)


`@part2`
**Sklearn**
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(penalty='l1',C=1e8)

clf.fit(X_train,y_train)
```
![sklearn](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20penalty%20c%20fit.png)


`@script`



---
## Sklearn intercept

```yaml
type: "FullSlide"
key: "6da452035f"
```

`@part1`
```
clf.intercept_
```

![sklearn intercept](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20intercept.png)


`@script`



---
## Statsmodel or Sklearn

```yaml
type: "FullSlide"
key: "e8209b89ce"
```

`@part1`
1. **It does not matter**
2. Results vary because of different parameter options used by the packages
3. The same reason applies when you compare the python output to SAS, R or any other software output  

_**Conclusion:**_ Understand the differences and stick to one{{1}}


`@script`



---
## Cool. what about categorical variables?

```yaml
type: "FullCodeSlide"
key: "1e52de7f88"
```

`@part1`
In the dataset, there is a categorical variable _**marital** _ with levels - _single, married, divorced_. One of the below option can be used to convert categorical to continuous.

1. Label encoder -  1,2,3 instead of single, married and divorced. {{1}}
2. One hot encoder - Creates dummy variable for each level.{{2}} 
3. Weight of evidence - Creates a index value based on distribution of target and non-target.{{3}}


`@script`



---
## Parameter estimates and odds ratio (Next)

```yaml
type: "FinalSlide"
key: "cbd484c536"
```

`@script`


