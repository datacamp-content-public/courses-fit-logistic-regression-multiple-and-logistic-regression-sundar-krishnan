---
title: Insert title here
key: 407f1f66af858cc19e8b121be75997e9

---
## Fit Logistic Regression

```yaml
type: "TitleSlide"
key: "6daa5ec860"
```

`@lower_third`

name: Sundar Krishnan
title: Lead Data Scientist


`@script`
In the previous lecture, we went through the concept of logistic regression and how it can be used to predict binary outcomes. In this lecture, we will understand how to write a python code to train a logistic regression model.


---
## Data

```yaml
type: "FullCodeSlide"
key: "d595d3eee5"
```

`@part1`
**Load Data**{{1}}
```
import pandas as pd
df = pd.read_excel("bank.xlsx")
```
{{1}}

**Sneak peek of the data**{{2}}
```
df.head()
```{{2}}
![df.head](https://assets.datacamp.com/production/repositories/4259/datasets/0f8cbbc36359b5f1c049263dad0972fa9e0cc4e3/Data%20-%20Sneak%20Peek.png){{2}}


`@script`
To load the data into python, we use pandas 'read_excel' option as shown here. Once loaded, we can use the df.head() function to take a sneak peek look of the data. Note that the target column 'y' is in yes or no format.


---
## More Information about the data

```yaml
type: "FullCodeSlide"
key: "044e6d48e9"
```

`@part1`
```
df.info()
```{{1}}
![df.info](https://assets.datacamp.com/production/repositories/4259/datasets/3857590bbd1389cf1ed2a6510265b253a5ff95aa/Data%20Information.png){{1}}


`@script`
That is cool, but what if I need more detailed information about the data. You can use df.info() function to do that.


---
## What we need to predict?

```yaml
type: "FullCodeSlide"
key: "700714b05b"
```

`@part1`
**Predict** - Whether the client subscribed for a term deposit (yes/no)?

{{1}}
```
df['target'] = df['y'].apply(lambda x: 1 if x == 'yes' else 0)
```{{1}}

**Count of each levels**{{2}}
```
df.target.value_counts()
```{{2}}
**Drop the categorical target column**{{3}}
```
df.drop('y',axis=1,inplace=True)
```{{3}}


`@script`
In this tutorial, we are predicting whether a client subscribes for a term deposit or not. A little preparation is needed for our target column here. Let us convert, the yes/no level to 1 and 0 respectively. Once it is done, we can take a look at the distribution of 1 and 0 using value_counts() function. Finally, we can drop the original target column 'y' since it is no longer needed.


---
## Packages to fit logistic regression

```yaml
type: "FullSlide"
key: "c3ab27aa49"
```

`@part1`
1. Statsmodels
2. Sklearn (Widely used)

**Note:** The input data should be **continuous**.{{1}}


`@script`
In python, we can use two packages to train a logistic regression model. The first one is statsmodels package and second one is the widely used sklearn package. Note that the input variables to both the packages will accept only continuous variables. For now, let us use the continuous variables in our data to build a logistic regression model.


---
## Train/Test split (Honest Assessment)

```yaml
type: "FullCodeSlide"
key: "e1e204df6d"
```

`@part1`
```
from sklearn.model_selection import train_test_split

X = df[['age','balance','day','duration','campaign','pdays','previous']]
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, 
random_state=42)
```{{1}}

The train and test dataset has 3,029 and 1,492 observations respectively.{{2}}
```
len(X_train)
len(X_test)
```{{2}}


`@script`
Before we build our model, we need to split our data to train and test datasets. The train data is used to develop model and the test data is used to evaluate performance of a model. This type of assessment is called Honest assessment. It is useful when we need to deploy a model in production. Notice that, we use a random_state to split our data. This is a seed number which is used to replicate our split results every time. After the split, we have roughly 3,000 observation in the training and 1500 observation in the test dataset.


---
## Statsmodel fit

```yaml
type: "FullCodeSlide"
key: "c3fa5f548a"
```

`@part1`
```
import statsmodels.api as sm

logit = sm.Logit(y_train, X_train).fit()
logit.summary()
```{{1}}
![statsmodel](https://github.com/Sundar0989/datacamp/blob/master/Statsmodels%20Logit%20fit.png){{1}}


`@script`
The statsmodel logistic regression fit function is shown here. Here it uses the statsmodels logit function to fit the model. Once the model is fit, the summary function is used to view the parameter estimates which we will in detail in the next video.


---
## Sklearn fit

```yaml
type: "FullCodeSlide"
key: "57bc1a5edb"
```

`@part1`
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
clf.fit(X_train,y_train)
```{{1}}

```
clf.coef_
```{{2}}

```
import numpy as np
pd.DataFrame(np.transpose(clf.coef_),index=X_train.columns,columns=['coef'])
```{{2}}
![sklearn](https://github.com/Sundar0989/datacamp/blob/master/Sklearn%20logit%20fit.png){{2}}


`@script`
You can also build a logistic regression model using sklearn package. Here, we define a logistic regression classifier function and assign the clf variable. Then we use the clf variable to fit the model. To look at the coefficients in sklearn, we need to use clf.coef_ feature as shown here. Finally, we use a little code to tie our coefficients to the input variables to make the output look in a nicer table format.


---
## Comparison of Fit outputs

```yaml
type: "TwoColumns"
key: "6c03b8f7d0"
```

`@part1`
**Statsmodel Coef**

![statsmodel](https://github.com/Sundar0989/datacamp/blob/master/Statsmodel%20logit%20coef.png)


`@part2`
**Sklearn Coef**

![sklearn](https://github.com/Sundar0989/datacamp/blob/master/Sklearn%20logit%20fit.png)


`@script`
Let us look at the two package outputs here. Notice that, the coefficients are different. Why does this happen?


---
## Why the fit coefficients are different?

```yaml
type: "FullSlide"
key: "3b3cf8593a"
```

`@part1`
By default, _sklearn_ fits an intercept which _statsmodel_ package does not. In addition, sklearn uses regularization (L2) by default.

![fit_intercept](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20fit%20intercept.png)

To bring consistency, one of these options will work{{1}}

1. Set **fit_intercept=False** in sklearn model (This is not recommended){{1}}
2. Add a **constant** in the X_train dataset before fitting statsmodel{{1}}
3. Using a large C value or change the regularization method or try both{{1}}


`@script`
By default, sklearn fits an intercept which statsmodel package does not. In addition, sklearn uses regularization by default. To bring consistency, one of these options will work. 1. Set **fit_intercept=False** in sklearn model. This option is not recommended. 2. Add a **constant** in the X_train dataset before fitting statsmodel. 3. Use a large C value or change the regularization method or try both. We will go through each of these option.


---
## Sklearn model without fit intercept (Option 1)

```yaml
type: "TwoColumns"
key: "a05fc5b150"
```

`@part1`
**Statsmodel fit**
```
import statsmodels.api as sm

logit = sm.Logit(y_train, X_train).fit()
logit.summary()
```
![statsmodel](https://github.com/Sundar0989/datacamp/blob/master/Statsmodel%20logit%20coef.png)


`@part2`
**sklearn fit without intercept**
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(fit_intercept=False)

clf.fit(X_train,y_train)
```
![sklearn](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20fit%20intercept%20false.png)


`@script`
After you set the fit_intercept to false the coefficients are same. This is cool. But is not recommended because, as I explained earlier in Linear regression, without an intercept, you always force the fit function to pass through origin (0,0) which wont provide the best fit.


---
## Updated statsmodel fit and sklearn model (Option 2 and 3)

```yaml
type: "TwoColumns"
key: "c20a35a6ed"
```

`@part1`
**Statsmodel**
```
import statsmodels.api as sm

new_train = sm.add_constant(X_train)
logit = sm.Logit(y_train, new_train).fit()
logit.summary()
```
![statsmdoel](https://github.com/Sundar0989/datacamp/blob/master/statsmodel%20const%20fit%20logit.png)


`@part2`
**Sklearn**
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(penalty='l1',C=1e8)

clf.fit(X_train,y_train)
```
![sklearn](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20penalty%20c%20fit.png)


`@script`
The best option is to add a constant '1' before fitting the statsmodel. This is established by add_constant function. But this result is also different from sklearn output. If you remember earlier, we did the same exercise in Linear regression and we got a match in the coefficient. But in logistic regression it is little different because of the penalty function also known as the regularization function. We can simulataneously tweak our sklearn function as shown on the right by changing the penalty and C parameter to get the result as close to statsmodel. Okay, everything is matching except the const variables in the statsmodel output. This is nothing but the intercept value.


---
## Sklearn intercept

```yaml
type: "FullSlide"
key: "6da452035f"
```

`@part1`
```
clf.intercept_
```

![sklearn intercept](https://github.com/Sundar0989/datacamp/blob/master/sklearn%20intercept.png)


`@script`
In sklearn you can access the intercept value using the code shown here. This value is very close to the statsmodel const output. Awesome. Now, we have looked at different fit options and their difference. So which one should we use?


---
## Statsmodel or Sklearn

```yaml
type: "FullSlide"
key: "e8209b89ce"
```

`@part1`
1. **It does not matter**
2. Results vary because of different parameter options used by the packages
3. The same reason applies when you compare the python output to SAS, R or any other software output  

_**Conclusion:**_ Understand the differences and stick to one{{1}}


`@script`
Technically, it does not matter. The results vary because both the package use different parameter options. In addition, let us say you used a different software tool kit like SAS or R or any other software and if the result is different it might be due to the same reason we discussed here. So what is the takeaway here. Understand the differences in packages and stick to one.


---
## Cool. what about categorical variables?

```yaml
type: "FullCodeSlide"
key: "1e52de7f88"
```

`@part1`
In the dataset, there is a categorical variable _**marital** _ with levels - _single, married, divorced_. One of the below option can be used to convert categorical to continuous.

1. Label encoder -  1,2,3 instead of single, married and divorced. {{1}}
2. One hot encoder - Creates dummy variable for each level.{{2}} 
3. [Weight of evidence](https://medium.com/@sundarstyles89/weight-of-evidence-and-information-value-using-python-6f05072e83eb) - Creates a index value based on distribution of target and non-target.{{3}}


`@script`
So far we used continuous variable to build our model. Okay, what if need to use a categorical variable. Let us take an example of marital status which has 3 levels - single, married and divorced. We can convert the categorical variable to continuous using one of the options below. Label encoder takes the categorical variable and assigns a random number for each levels. In this case, the single, married and divorced would be assigned 1,2 and 3. Now, this can be used in logistic regression model directly as a continuous feature. However, this method has a disadvantage during interpretation of the final outcome which will be discussed later. Another method is to create dummy variable for each level. So there will be 1/0 variable for each category. This method overcomes the disadvantage faced in the earlier method. However, it has its own disadvantage. It creates a redundant variable in the process. If we have a 1/0 variable for each of the levels single and married, there is no need for a third variable for divorced category. The final option is to use Weight of evidence. This option creates a index value based on the distribution of 1 in each level vs the distribution of 0 in each level. To read more about Weight of evidence, you can refer to the article attached to the link.


---
## Parameter estimates and odds ratio (Next)

```yaml
type: "FinalSlide"
key: "cbd484c536"
```

`@script`
Wow!!! We build our regression model using two different packages. In the next exercise, we will understand how to interpret the coefficients of the model and odds ratio. See you then.

