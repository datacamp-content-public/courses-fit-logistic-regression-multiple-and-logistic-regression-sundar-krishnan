---
title: Insert title here
key: 407f1f66af858cc19e8b121be75997e9

---
## Fit Logistic Regression

```yaml
type: "TitleSlide"
key: "6daa5ec860"
```

`@lower_third`

name: Sundar Krishnan
title: Data Scientist


`@script`
In this lecture, we will understand how to write a python code to train a logistic regression model.


---
## Packages to fit logistic regression

```yaml
type: "FullSlide"
key: "c3ab27aa49"
```

`@part1`
1. Statsmodels
2. Sklearn

**Note:** _Sklearn_ can only use continuous input variables.{{1}}

**Numeric variables in the data**{{2}}

```
X_num = X_train[['age','balance','duration']]
```{{2}}


`@script`
In python, we can use two packages to train a logistic regression model. The first one is statsmodels and the second one is sklearn package. Note that, sklearn can only use continuous input variables to train the model. To compare the output results of both these packages, we will only use numeric variables for now.


---
## Statsmodel fit

```yaml
type: "FullCodeSlide"
key: "c3fa5f548a"
```

`@part1`
```
import statsmodels.api as sm

logit = sm.Logit(y_train, X_num).fit()
pd.DataFrame(logit.params, columns=['coef'])
```{{1}}
![statsmodel](https://assets.datacamp.com/production/repositories/4259/datasets/2562635201d76e19f99f731cac58a712924d6042/statsmodel%20without%20logit.png){{1}}


`@script`
The statsmodel "logit" option is used to fit logistic regression model. Once the model is fit, the params option is used to view the parameter estimates.


---
## Sklearn fit

```yaml
type: "FullCodeSlide"
key: "57bc1a5edb"
```

`@part1`
```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
clf.fit(X_num,y_train)
```{{1}}
```
import numpy as np
pd.DataFrame(np.transpose(clf.coef_), index=X_num.columns, columns=['coef'])
```{{2}}
![sklearn](https://assets.datacamp.com/production/repositories/4259/datasets/1899edffcb1bfa3c137a35ddd98f7ee9f735c13a/sklearn%20initial%20fit.png){{2}}


`@script`
You can also build a logistic regression model using sklearn package. Here, we define a logistic regression classifier function and assign it to the clf variable. Then we use the clf variable to fit the model. To look at the parameter estimates in sklearn, we need to use clf.coef_ feature as shown here.


---
## Comparison of Fit outputs

```yaml
type: "TwoColumns"
key: "6c03b8f7d0"
```

`@part1`
**Statsmodel Coef**

![statsmodel](https://assets.datacamp.com/production/repositories/4259/datasets/2562635201d76e19f99f731cac58a712924d6042/statsmodel%20without%20logit.png)


`@part2`
**Sklearn Coef**

![sklearn](https://assets.datacamp.com/production/repositories/4259/datasets/1899edffcb1bfa3c137a35ddd98f7ee9f735c13a/sklearn%20initial%20fit.png)


`@script`
Let us look at the two package outputs here. Notice that, the coefficients are different. Why does this happen?


---
## Why the fit coefficients are different?

```yaml
type: "FullSlide"
key: "3b3cf8593a"
```

`@part1`
- **_sklearn_** fits an intercept, **_statsmodel_** package does not. 
- _**sklearn**_ uses l2 regularization (penalty='l2') by default.

![fit_intercept](https://assets.datacamp.com/production/repositories/4259/datasets/1fdc94478fe2a40e8d05e071e87e0967e3a2d14a/sklearn%20default%20settings.png)

To bring consistency, one of these options will work{{1}}

1. Set **fit_intercept=False** in _**sklearn**_ model (This is not recommended){{1}} 
2. Add a **constant** in the X_num dataset before fitting _**statsmodel**_{{1}}
3. Using a **large C** value or change the **regularization** method in _**sklearn**_ or try both{{1}}


`@script`
By default, sklearn fits an intercept whereas statsmodel package does not. In addition, sklearn uses l2 regularization by default. To bring consistency, one of these options will work. 1. Set **fit_intercept=False** in sklearn model. This option is not recommended. 2. Add a **constant** in the train (X_num) dataset before fitting statsmodel. 3. Use a large C value or change the regularization method in sklearn or try both. We will go through each of these option.


---
## Sklearn model without fit intercept (Option 1)

```yaml
type: "TwoColumns"
key: "a05fc5b150"
```

`@part1`
**Statsmodel fit**
```
logit = sm.Logit(y_train, X_num).fit()
pd.DataFrame(logit.params, columns=['coef'])
```
![statsmodel](https://assets.datacamp.com/production/repositories/4259/datasets/2562635201d76e19f99f731cac58a712924d6042/statsmodel%20without%20logit.png)


`@part2`
**sklearn fit without intercept**
```
clf = LogisticRegression(fit_intercept=False)
clf.fit(X_num,y_train)
pd.DataFrame(np.transpose(clf.coef_), index=X_num.columns, columns=['coef'])
```
![sklearn](https://assets.datacamp.com/production/repositories/4259/datasets/b4d93108b688bcc04bb2df17d40a2f6a2c41f3ac/sklearn%20fit%20intercept%20false.png)


`@script`
After you set the fit_intercept to false, the coefficients are almost same. This is cool. But is not recommended because, as I explained earlier in Linear regression, without an intercept, you always force the fit function to pass through origin (0,0) which wont provide the best fit. So what is the alternative.


---
## Updated statsmodel fit (Option 2)

```yaml
type: "TwoColumns"
key: "c20a35a6ed"
```

`@part1`
**Statsmodel**
```
new_train = sm.add_constant(X_num)
logit = sm.Logit(y_train, new_train).fit()
pd.DataFrame(logit.params, columns=['coef'])
```
![statsmdoel](https://assets.datacamp.com/production/repositories/4259/datasets/62c77a5fef2852877af0a8c1b1cc70d8dd5ca970/statsmodel%20with%20constant.png)


`@part2`
**Sklearn**
```
clf = LogisticRegression()
clf.fit(X_num,y_train)
pd.DataFrame(np.transpose(clf.coef_), index=X_num.columns, columns=['coef'])
```
![sklearn](https://assets.datacamp.com/production/repositories/4259/datasets/1899edffcb1bfa3c137a35ddd98f7ee9f735c13a/sklearn%20initial%20fit.png)


`@script`
The best option is to add a constant '1' before fitting the statsmodel. This is established by add_constant function. But this result is also different from sklearn output. If you remember earlier, we did the same exercise in Linear regression and we got a match in the coefficient. But in logistic regression it is little different. So, let us look at our option 2 and 3 simultaneously.


---
## Updated statsmodel and sklearn fit (Option 2 and 3)

```yaml
type: "TwoColumns"
key: "e59c9a60a1"
disable_transition: true
```

`@part1`
**Statsmodel**
```
new_train = sm.add_constant(X_num)
logit = sm.Logit(y_train, new_train).fit()
pd.DataFrame(logit.params, columns=['coef'])
```
![statsmdoel](https://assets.datacamp.com/production/repositories/4259/datasets/62c77a5fef2852877af0a8c1b1cc70d8dd5ca970/statsmodel%20with%20constant.png)


`@part2`
**Sklearn**
```
clf = LogisticRegression(C=1e8,penalty='l1')
clf.fit(X_num,y_train)
pd.DataFrame(np.transpose(clf.coef_), index=X_num.columns, columns=['coef'])
```
![sklearn](https://assets.datacamp.com/production/repositories/4259/datasets/5eb9034f513e5af5f68ff44bc42813af1c757c42/sklearn%20with%20large%20C.png)


`@script`
We tweak the sklearn function as shown on the right by changing the penalty to l1 and adding a large C parameter to get the result as close to statsmodel. Okay, everything is matching except the "const" variable in the statsmodel output. This is the intercept value.


---
## Sklearn intercept

```yaml
type: "FullSlide"
key: "6da452035f"
```

`@part1`
```
clf.intercept_
```

![sklearn intercept](https://assets.datacamp.com/production/repositories/4259/datasets/218767e470e09b2523cdb82de32ba33ac569d28c/sklearn%20intercept.png)


`@script`
In sklearn, you can access the intercept value using the code shown here. This value is very close to the statsmodel const output. Awesome. Now, we have looked at different fit options and their difference. So which one should we use?


---
## Statsmodel or Sklearn

```yaml
type: "FullSlide"
key: "e8209b89ce"
```

`@part1`
1. **It does not matter**
2. Results vary because of different parameter options used by the packages
3. The same reason applies when you compare the python output to SAS, R or any other software output  

_**Conclusion:**_ Understand the differences and stick to one{{1}}


`@script`
Technically, it does not matter. The results vary because both the package use different parameter options. In addition, let us say you used a different software tool kit like SAS or R or any other software and if the result is different, it might be due to the same reason we discussed here. So what is the takeaway here. Understand the differences in packages and stick to one.


---
## Let's Practice

```yaml
type: "FinalSlide"
key: "cbd484c536"
```

`@script`
Wow!!! We build our regression model using two different packages. Now it is time to practice.

